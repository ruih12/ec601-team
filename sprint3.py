# -*- coding: utf-8 -*-
"""sprint3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15OW_V8s3ODYVEiDHizhHh37Z0DJQQP6K

# **museum_historical_animation**

**dataset**                      # 图片数据集目录
historical_figures       # 历史人物图片集
reference_videos         # 参考视频（用于运动迁移）

**preprocess.py**                 # 数据预处理脚本
功能：对历史人物图像进行对齐和裁剪，增强模型生成效果

**model_training**               # 训练模块
train_stylegan.py         # 训练StyleGAN生成高清图像
train_motion_model.py     # 训练First Order Motion Model
功能：实现高质量图像生成与动态合成

**generate_animation.py**         # 生成动图的主脚本
功能：根据历史人物图像和参考视频生成最终动态效果

**utils**                        # 辅助工具
align_faces.py            # 面部对齐工具
video_post_processing.py  # 视频后处理工具
"""

!git clone https://github.com/ruih12/ec601-team.git

"""# 1. preprocess.py: Data Preprocessing Script
Purpose: The preprocessing script is used to align and crop facial images to enhance model performance during training and animation generation. By standardizing face alignment, the model can more easily recognize facial features and apply transformations consistently.

Implementation: In this code, we use the Dlib library for facial detection and feature extraction, specifically focusing on identifying the position of key facial landmarks, particularly the eyes, which are used to align the face.

Code Structure and Workflow:

Face Detection: detector is used to locate faces within images.
Facial Landmark Detection: predictor identifies 68 specific points on each face, allowing precise extraction of facial landmarks.
Face Alignment: Once the positions of the eyes are known, the code calculates the angle between them, then rotates the face to standardize orientation.
Image Transformation: An affine transformation is applied to each face using OpenCV’s warpAffine function, ensuring consistent alignment for each image.
Potential Improvements:

Augmentation: Additional preprocessing steps such as brightness, contrast, and noise adjustment could be applied to make the model more robust.
Masking: Applying a facial mask could further refine alignment, focusing the model’s attention on the face rather than the background.
"""

# preprocess.py

import os
import cv2
import dlib
import numpy as np

# 初始化人脸检测器和预测器
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

def preprocess_images(input_dir, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for img_name in os.listdir(input_dir):
        img_path = os.path.join(input_dir, img_name)
        image = cv2.imread(img_path)
        if image is None:
            continue

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = detector(gray)

        for face in faces:
            landmarks = predictor(gray, face)
            aligned_face = align_face(image, landmarks)
            output_path = os.path.join(output_dir, img_name)
            cv2.imwrite(output_path, aligned_face)
            break  # 只处理检测到的第一个人脸

def align_face(image, landmarks):
    # 提取眼睛的位置
    left_eye = (landmarks.part(36).x, landmarks.part(36).y)
    right_eye = (landmarks.part(45).x, landmarks.part(45).y)

    # 计算眼睛的中心点
    eyes_center = ((left_eye[0] + right_eye[0]) // 2,
                   (left_eye[1] + right_eye[1]) // 2)

    # 计算旋转角度
    dy = right_eye[1] - left_eye[1]
    dx = right_eye[0] - left_eye[0]
    angle = np.degrees(np.arctan2(dy, dx))

    # 构建旋转矩阵
    M = cv2.getRotationMatrix2D(eyes_center, angle, scale=1)

    # 应用仿射变换
    aligned_face = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]),
                                  flags=cv2.INTER_CUBIC)
    return aligned_face

if __name__ == '__main__':
    input_directory = 'dataset/historical_figures/'
    output_directory = 'dataset/preprocessed_figures/'
    preprocess_images(input_directory, output_directory)

"""# 2. model_training/: Training Modules
This directory contains training scripts for both StyleGAN and the First Order Motion Model, each addressing different aspects of high-quality image generation and motion transfer.

# 2.1 train_stylegan.py: Training the StyleGAN Model
Purpose: StyleGAN is used to generate high-quality facial images. In this context, it’s trained on historical figures’ faces to create realistic images that mimic a particular style or period.

Implementation: The script loads an image dataset, applies necessary transformations (resizing, cropping, normalization), and defines a StyleGAN-based generator model. The model is trained to recreate input images while applying subtle variations.

Code Structure:

Data Loader: Uses torchvision.datasets.ImageFolder to load and process images from the preprocessed dataset.
Generator Model: StyleGANGenerator (implemented in model.py) is used for generating images. It contains layers for deconvolution and non-linear activation, creating images from random noise.
Loss and Optimization: Uses Mean Squared Error (MSE) loss and Adam optimizer, which help the model learn subtle features over time. MSE is commonly chosen for image reconstruction.
Potential Improvements:

Conditional GAN: Adding conditions based on historical periods or individual characteristics (e.g., era, gender) could enhance style coherence.
Regularization: Adding additional loss functions, like perceptual loss, could improve image quality by encouraging more realistic textures and colors.
# 2.2 train_motion_model.py: Training the First Order Motion Model
Purpose: This model transfers motion from a video reference to a source image, generating animations that maintain the source’s identity while replicating the reference’s motion.

Implementation: The model is trained using both a dataset of static source images and driving videos with different motion patterns, learning to apply dynamic features onto static images.

Code Structure:

Data Loader: MotionDataset loads pairs of source images and reference videos.
Model Architecture: MotionTransferModel is a network that combines features from the source and driving images to create a synthetic video. The architecture includes convolutional layers to encode images and transposed convolutional layers to decode the generated frames.
Loss and Optimization: L1 loss is used to penalize differences between generated and driving videos, encouraging realistic motion transfer.
Potential Improvements:

Attention Mechanisms: Attention layers could focus the model on the most relevant parts of the face, like the eyes and mouth, which are most dynamic.
Temporal Consistency Loss: Adding a temporal loss function to maintain consistency between consecutive frames could reduce jitter.
"""

# train_stylegan.py

import os
import torch
from torchvision import datasets, transforms
from model import StyleGANGenerator

def train_stylegan(data_dir, epochs=100, batch_size=16, lr=0.001):
    # 数据预处理
    transform = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(256),
        transforms.ToTensor(),
    ])

    dataset = datasets.ImageFolder(data_dir, transform=transform)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # 初始化模型、优化器和损失函数
    model = StyleGANGenerator()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.MSELoss()

    model.train()
    for epoch in range(epochs):
        for i, (images, _) in enumerate(dataloader):
            images = images.cuda()

            # 前向传播
            outputs = model(images)
            loss = criterion(outputs, images)

            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if i % 100 == 0:
                print(f'Epoch [{epoch}/{epochs}], Step [{i}], Loss: {loss.item():.4f}')

    # 保存模型
    torch.save(model.state_dict(), 'stylegan_generator.pth')

if __name__ == '__main__':
    data_directory = 'dataset/preprocessed_figures/'
    train_stylegan(data_directory)

# train_motion_model.py

import torch
from torch.utils.data import DataLoader
from dataset import MotionDataset
from model import MotionTransferModel

def train_motion_model(image_dir, video_dir, epochs=50, batch_size=8, lr=0.0002):
    dataset = MotionDataset(image_dir, video_dir)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = MotionTransferModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = torch.nn.L1Loss()

    model.train()
    for epoch in range(epochs):
        for i, (source_image, driving_video) in enumerate(dataloader):
            source_image = source_image.cuda()
            driving_video = driving_video.cuda()

            # 前向传播
            generated_video = model(source_image, driving_video)
            loss = criterion(generated_video, driving_video)

            # 反向传播和优化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if i % 10 == 0:
                print(f'Epoch [{epoch}/{epochs}], Step [{i}], Loss: {loss.item():.4f}')

    # 保存模型
    torch.save(model.state_dict(), 'motion_transfer_model.pth')

if __name__ == '__main__':
    image_directory = 'dataset/preprocessed_figures/'
    video_directory = 'dataset/reference_videos/'
    train_motion_model(image_directory, video_directory)

"""# 3. generate_animation.py: Animation Generation Script
Purpose: The primary function of this script is to synthesize an animated video by combining a static image of a historical figure with a reference video.

Implementation: The model loads a pretrained motion transfer model and applies it to generate a sequence of frames that simulate the chosen movement patterns on the target image.

Code Structure:

Model Loading: The pretrained model weights are loaded, ensuring that previously trained patterns are applied.
Preprocessing: Image and video are resized and normalized before being passed into the model.
Animation Generation: The model combines the static image and motion to produce a dynamic sequence.
Output Storage: The resulting animation is saved using the save_video function, which consolidates frames into a video file.
Potential Improvements:

Adaptive Motion Scaling: Adding controls for adjusting motion intensity could make animations more adaptable to different character styles.
Real-Time Rendering: Optimizing the process for GPU could allow real-time generation, beneficial for interactive applications.
"""

# generate_animation.py

import torch
from torchvision import transforms
from model import MotionTransferModel
from utils import load_image, load_video, save_video

def generate_animation(image_path, video_path, output_path):
    # 加载模型
    model = MotionTransferModel()
    model.load_state_dict(torch.load('motion_transfer_model.pth'))
    model.eval()

    # 数据预处理
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
    ])

    source_image = load_image(image_path, transform).unsqueeze(0).cuda()
    driving_video = load_video(video_path, transform).unsqueeze(0).cuda()

    # 生成动画
    with torch.no_grad():
        generated_video = model(source_image, driving_video)

    # 保存结果
    save_video(generated_video, output_path)

if __name__ == '__main__':
    image_path = 'dataset/preprocessed_figures/person1.jpg'
    video_path = 'dataset/reference_videos/sample.mp4'
    output_path = 'output/person1_animation.mp4'
    generate_animation(image_path, video_path, output_path)

"""# 4. utils/: Utility Scripts
This directory includes auxiliary scripts for face alignment, video smoothing, and saving, providing essential support functions for preprocessing and post-processing.

# 4.1 align_faces.py: Face Alignment Tool
Purpose: Aligns and adjusts facial images for consistency across datasets, especially useful for images with varying head orientations.

Implementation: The script calculates the angle between the eyes to orient each image, ensuring faces are centered and properly aligned for later processing.

Structure:

Face Detection: Identifies faces in grayscale format, improving accuracy.
Angle Calculation: Computes angle from left to right eye to determine rotation.
Affine Transformation: Corrects face alignment via an affine transformation, preserving facial dimensions and alignment.
Potential Improvements:

Facial Masking: Masks could crop everything but the face, allowing more precise alignment in highly varied images.
Advanced Affine Transformations: Applying additional affine parameters could improve alignment for off-center faces.
## 4.2 video_post_processing.py: Video Smoothing and Enhancement Tool
Purpose: Smooths video frames to produce a seamless appearance, reducing flickering and artifacts common in generated videos.

Implementation: Uses temporal averaging, applying a sliding window of frames to calculate a smoother transition between frames.

Structure:

Frame Averaging: Averages a range of frames around each target frame to reduce abrupt changes.
Video Saving: Saves the smoothed frames as a new video file, ensuring compatibility with the video format and frame rate.
Potential Improvements:

Adaptive Smoothing Window: Using variable window sizes depending on the amount of motion could further improve output quality.
Post-Processing Filters: Additional filters like sharpening or color correction could be added to enhance realism.
"""

# utils/align_faces.py

import cv2
import dlib
import numpy as np

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

def align_face(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    if len(faces) == 0:
        return None

    face = faces[0]
    landmarks = predictor(gray, face)

    # 提取眼睛的位置
    left_eye = np.array([landmarks.part(36).x, landmarks.part(36).y])
    right_eye = np.array([landmarks.part(45).x, landmarks.part(45).y])

    # 计算眼睛的中心点
    eyes_center = (left_eye + right_eye) / 2.0

    # 计算旋转角度
    dy = right_eye[1] - left_eye[1]
    dx = right_eye[0] - left_eye[0]
    angle = np.degrees(np.arctan2(dy, dx))

    # 计算仿射变换矩阵
    M = cv2.getRotationMatrix2D(tuple(eyes_center), angle, scale=1)

    # 应用仿射变换
    aligned_face = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]),
                                  flags=cv2.INTER_CUBIC)

    return aligned_face

# utils/video_post_processing.py

import cv2
import numpy as np

def smooth_video(input_video_path, output_video_path, smoothing_window=5):
    cap = cv2.VideoCapture(input_video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()

    smoothed_frames = []
    for i in range(len(frames)):
        avg_frame = np.zeros_like(frames[i], dtype=np.float32)
        count = 0
        for j in range(max(0, i - smoothing_window), min(len(frames), i + smoothing_window)):
            avg_frame += frames[j].astype(np.float32)
            count += 1
        avg_frame /= count
        smoothed_frames.append(avg_frame.astype(np.uint8))

    # 保存平滑后的视频
    height, width, layers = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, 25.0, (width, height))

    for frame in smoothed_frames:
        out.write(frame)
    out.release()

def enhance_video(input_video_path, output_video_path):
    # 可以在这里添加更多的后处理，如颜色增强、锐化等
    pass

"""# 5. utils/__init__.py: Utility Functions
This script provides helper functions for loading, processing, and saving images and videos, used in multiple parts of the project.

Key Components:
load_image: Loads and preprocesses images, ensuring they’re in RGB format and compatible with model inputs.
load_video: Loads videos frame by frame, converting each to a tensor.
save_video: Compiles generated frames into a video file, ensuring proper RGB-to-BGR conversion and saving in a chosen format (MP4).
Potential Improvements:

Efficient Loading: Using parallel loading for video frames could speed up the process, especially useful for high-resolution videos.
Customizable Save Options: Allowing adjustable output formats and resolutions could make the function more versatile.

"""

# utils/__init__.py

import cv2
import torch
import numpy as np
from torchvision import transforms

def load_image(image_path, transform=None):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    if transform:
        image = transform(image)
    return image

def load_video(video_path, transform=None):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        if transform:
            frame = transform(frame)
        frames.append(frame)
    cap.release()
    video = torch.stack(frames)
    return video

def save_video(frames, output_path, fps=25):
    frames = frames.cpu().numpy()
    frames = (frames * 255).astype(np.uint8)
    height, width = frames.shape[2], frames.shape[3]
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    for frame in frames:
        frame = np.transpose(frame, (1, 2, 0))
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
        out.write(frame)
    out.release()

"""# 6. model.py: Model Definitions
This script defines the core neural networks: the StyleGANGenerator and the MotionTransferModel, each responsible for a distinct aspect of the animation process.

# 6.1 StyleGANGenerator
Purpose: Generates high-quality images, adapting StyleGAN architecture to historical figure features.

Implementation: Uses a series of transpose convolutional layers, with each layer progressively increasing image resolution.

Structure:

Generator Layers: Each layer doubles the resolution until the final image size is achieved, using ReLU activation functions to add non-linearity.
Output Layer: The final layer uses Tanh activation, scaling pixel values between -1 and 1 for compatibility with standard preprocessing practices.
Potential Improvements:

Progressive Growing of GANs: Incorporating progressively growing layers, as in StyleGAN2, could yield better details and style coherence.
Conditional GAN Extension: Adding conditions based on attributes (like age or historical period) could further enhance image relevance.
# 6.2 MotionTransferModel
Purpose: Combines features from a source image and driving video to produce realistic animations.

Implementation: The model uses an encoder-decoder architecture, where the encoder extracts features from both source and driving video, and the decoder combines these features into new frames.

Structure:

Encoder: Convolutional layers extract features, while ReLU activations introduce non-linearities.
Decoder: Transpose convolutional layers convert encoded features back into the spatial domain, creating frames from feature maps.
Potential Improvements:

Feature Fusion: Employing more sophisticated feature fusion techniques, like cross-attention layers, could enhance motion transfer fidelity.
Temporal Coherence Layers: Layers that ensure temporal coherence between frames could smooth animations and reduce flickering.
"""

# model.py

import torch
import torch.nn as nn

class StyleGANGenerator(nn.Module):
    def __init__(self):
        super(StyleGANGenerator, self).__init__()
        # 简化的生成器结构
        self.main = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 1, 0),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, x):
        return self.main(x)

class MotionTransferModel(nn.Module):
    def __init__(self):
        super(MotionTransferModel, self).__init__()
        # 简化的运动迁移模型
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),
            nn.ReLU(True),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.ReLU(True),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1),
            nn.Tanh()
        )

    def forward(self, source_image, driving_video):
        # 简化的前向传播，将源图像和驱动视频的特征融合
        source_feature = self.encoder(source_image)
        driving_feature = self.encoder(driving_video)

        combined_feature = source_feature + driving_feature
        output = self.decoder(combined_feature)
        return output

"""# 7. dataset.py: Custom Dataset Loader
Purpose: Loads paired datasets of static source images and dynamic driving videos, organizing them into an accessible structure for training.

Implementation: The MotionDataset class iterates through image and video directories, loading corresponding items.

Structure:

__getitem__: Returns a source image and a driving video frame sequence.
__len__: Ensures the dataset length is the smaller of the two directories’ sizes, maintaining equal pairs.
Potential Improvements:

Dynamic Pairing: Randomly pairing images with videos for each epoch could improve the model’s ability to generalize.
Balanced Data Sampling: Implementing sampling strategies to balance the distribution of motion types could prevent bias toward specific motions.

"""

# dataset.py

import os
import torch
from torch.utils.data import Dataset
from utils import load_image, load_video

class MotionDataset(Dataset):
    def __init__(self, image_dir, video_dir, transform=None):
        self.image_dir = image_dir
        self.video_dir = video_dir
        self.transform = transform

        self.image_files = os.listdir(image_dir)
        self.video_files = os.listdir(video_dir)

    def __len__(self):
        return min(len(self.image_files), len(self.video_files))

    def __getitem__(self, idx):
        image_path = os.path.join(self.image_dir, self.image_files[idx])
        video_path = os.path.join(self.video_dir, self.video_files[idx])

        source_image = load_image(image_path, self.transform)
        driving_video = load_video(video_path, self.transform)

        return source_image, driving_video