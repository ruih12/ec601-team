{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhQ37vBZLR8AsHdL22YXGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruih12/ec601-team/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Perceptual Loss for Regularization\n",
        "Perceptual loss encourages the model to generate images that are not only pixel-wise accurate but also perceptually similar to real images by focusing on high-level feature representations. This loss can enhance the texture and color fidelity of generated images.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "sJ7dBSjAFofS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSXTpQ85FXPI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        vgg = models.vgg19(pretrained=True).features\n",
        "        self.layers = nn.Sequential(*list(vgg[:36])).eval()  # Use the first 36 layers of VGG19\n",
        "        for param in self.layers.parameters():\n",
        "            param.requires_grad = False  # Freeze VGG19 layers\n",
        "\n",
        "    def forward(self, generated, target):\n",
        "        gen_features = self.layers(generated)\n",
        "        target_features = self.layers(target)\n",
        "        perceptual_loss = nn.functional.l1_loss(gen_features, target_features)\n",
        "        return perceptual_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "VGG19 Feature Extractor: We use a pretrained VGG19 model to extract features from the generated and target images. The feature extractor is truncated at the 36th layer, capturing deeper and more detailed features for perceptual comparison.\n",
        "\n",
        "Feature Extraction: For each forward pass, the generated and target images are passed through the VGG19 layers, producing feature maps.\n",
        "\n",
        "Loss Calculation: The perceptual loss is the L1 loss between the feature maps of the generated and target images. This loss ensures that generated images match high-level features, improving texture and detail."
      ],
      "metadata": {
        "id": "MqXtWRtrFtYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integration in Training**: Add this loss in the main training loop alongside other losses:"
      ],
      "metadata": {
        "id": "tZ6Y8ZztFuq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate perceptual loss\n",
        "perceptual_loss_fn = PerceptualLoss().cuda()\n",
        "\n",
        "# In the training loop\n",
        "for images, targets in dataloader:\n",
        "    images, targets = images.cuda(), targets.cuda()\n",
        "\n",
        "    generated_images = model(images)\n",
        "    perceptual_loss = perceptual_loss_fn(generated_images, targets)\n",
        "\n",
        "    total_loss = perceptual_loss + other_losses  # Combine with other loss terms\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "5KILmiQzFwx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Attention Mechanism\n",
        "Adding attention layers in the modelâ€™s architecture, particularly around dynamic areas such as the eyes and mouth, allows the model to focus on these regions more effectively. Below is a simple spatial attention mechanism added to the generator model."
      ],
      "metadata": {
        "id": "r5c2_CBzGAwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, height, width = x.size()\n",
        "        query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch, -1, width * height)\n",
        "        attention = self.softmax(torch.bmm(query, key))\n",
        "        value = self.value_conv(x).view(batch, -1, width * height)\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch, channels, height, width)\n",
        "        return out + x  # Residual connection\n",
        "\n",
        "class EnhancedGenerator(nn.Module):\n",
        "    def __init__(self, base_generator):\n",
        "        super(EnhancedGenerator, self).__init__()\n",
        "        self.base = base_generator\n",
        "        self.attention = AttentionLayer(in_channels=64)  # Add attention to a specific layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base.layer1(x)  # Replace with actual base layers\n",
        "        x = self.attention(x)\n",
        "        x = self.base.layer2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "EVwjBxm9GCSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Query, Key, and Value Convolutions: The attention layer creates three matrices (query, key, value) from the input feature map. These matrices represent different perspectives of the same data.\n",
        "\n",
        "Softmax Attention: Calculates a similarity matrix (attention map) between query and key. This map weights the importance of different regions.\n",
        "\n",
        "Attention Output: The weighted sum of value and attention map produces a refined output with focused attention on dynamic regions like eyes and mouth. This output is added to the original input as a residual connection to retain base features.\n",
        "\n",
        "Integration in Generator Model: Wrap the base generator with this attention layer and incorporate it into the main model architecture."
      ],
      "metadata": {
        "id": "8JZRaiMPGDuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Temporal Consistency Loss\n",
        "Temporal consistency loss encourages coherence between consecutive frames in the generated video, reducing jitter and ensuring smoother transitions across frames.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "q6Qe3BKxGZFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalConsistencyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TemporalConsistencyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, current_frame, previous_frame):\n",
        "        # Compute L1 loss between the consecutive frames\n",
        "        return nn.functional.l1_loss(current_frame, previous_frame)\n",
        "\n",
        "# In the training loop for video generation\n",
        "temporal_loss_fn = TemporalConsistencyLoss()\n",
        "\n",
        "# Assume frames is a list of generated frames\n",
        "temporal_loss = 0.0\n",
        "for i in range(1, len(frames)):\n",
        "    temporal_loss += temporal_loss_fn(frames[i], frames[i-1])  # Loss between consecutive frames\n",
        "\n",
        "total_loss = temporal_loss + other_losses\n"
      ],
      "metadata": {
        "id": "Oi0lpRXiGbOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Temporal Consistency Loss: Measures the L1 loss between consecutive frames, penalizing sudden changes. This loss encourages smooth transitions, reducing flicker in generated videos.\n",
        "\n",
        "Summing Temporal Loss Across Frames: Accumulate the loss over all consecutive frame pairs in a generated sequence to ensure consistent changes over time.\n",
        "\n",
        "Integration in Training Loop: The temporal loss is added alongside other losses. For video sequences, it should be applied to every consecutive frame pair in a mini-batch of generated frames."
      ],
      "metadata": {
        "id": "igcIxajZGfMN"
      }
    }
  ]
}