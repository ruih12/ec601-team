{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR7d1RwBcYo9YDdY1sQcvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruih12/ec601-team/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Progressive Growing Layers\n",
        "Progressive growing, used in StyleGAN2, allows the model to learn finer details at increasing resolutions. This approach starts with lower resolution images and gradually grows the image size, adding layers incrementally during training. This process can improve stability and detail consistency.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "FjvvyE41KXJC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJig_6orJnHq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProgressiveGrowingGenerator(nn.Module):\n",
        "    def __init__(self, initial_resolution=4, final_resolution=128, growth_factor=2):\n",
        "        super(ProgressiveGrowingGenerator, self).__init__()\n",
        "        self.current_resolution = initial_resolution\n",
        "        self.final_resolution = final_resolution\n",
        "        self.growth_factor = growth_factor\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        # Add the initial low-resolution block\n",
        "        self.blocks.append(self._create_block(initial_resolution))\n",
        "\n",
        "    def _create_block(self, resolution):\n",
        "        # Define a simple convolutional block for a given resolution\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def grow(self):\n",
        "        if self.current_resolution < self.final_resolution:\n",
        "            self.current_resolution *= self.growth_factor\n",
        "            self.blocks.append(self._create_block(self.current_resolution))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Initial Block: Starts at a low resolution (e.g., 4x4) to stabilize early training.\n",
        "\n",
        "Growing Mechanism: The grow() function adds new convolutional layers, doubling the image resolution in stages.\n",
        "\n",
        "Dynamic Resolutions: Each block progressively increases the detail as the resolution grows, allowing the model to refine image features iteratively.\n",
        "\n",
        "Integration in Training: Gradually call grow() during training to increase the resolution as the model stabilizes, eventually reaching the final resolution."
      ],
      "metadata": {
        "id": "vuS3SFBJKZlR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Conditional GAN Extension\n",
        "Conditional GANs (cGANs) improve image relevance and personalization by conditioning the generation process on auxiliary information such as age, historical period, or other attributes.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "rYbHkLsAKwkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, num_classes, latent_dim=100):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_classes, latent_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 3 * 128 * 128),  # Target image resolution\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_embedding = self.embedding(labels)\n",
        "        x = torch.cat([noise, label_embedding], dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x.view(-1, 3, 128, 128)\n"
      ],
      "metadata": {
        "id": "rdHZ3V-UKx1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Embedding Layer: The nn.Embedding layer converts class labels (e.g., age or period) into a dense vector that the generator can interpret.\n",
        "\n",
        "Concatenation with Noise Vector: The latent noise vector and label embedding are concatenated, allowing the model to conditionally generate images based on the specified attributes.\n",
        "\n",
        "Fully Connected Layers: Layers generate an image based on both noise and the specified condition, resulting in images that align with the given attributes.\n",
        "\n",
        "Integration in Training: Use labels for both real and generated images, feeding the same labels to the discriminator to help the model generate contextually accurate images."
      ],
      "metadata": {
        "id": "z8Z6Mdw-K1E3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Advanced Feature Fusion with Cross-Attention\n",
        "Using cross-attention for feature fusion can enhance the motion transfer’s realism by better aligning features between source and target images.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "BL1hdF7OK3YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, source_features, target_features):\n",
        "        # Generate query, key, and value matrices\n",
        "        query = self.query(source_features)\n",
        "        key = self.key(target_features)\n",
        "        value = self.value(target_features)\n",
        "\n",
        "        # Cross-attention mechanism\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "        attention_weights = self.softmax(attention_scores)\n",
        "        attended_features = torch.matmul(attention_weights, value)\n",
        "\n",
        "        return attended_features + source_features  # Residual connection\n",
        "\n",
        "class FeatureFusionModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(FeatureFusionModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.cross_attention = CrossAttention(dim=256)  # Example feature dimension\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source_features = self.base_model(source)\n",
        "        target_features = self.base_model(target)\n",
        "\n",
        "        fused_features = self.cross_attention(source_features, target_features)\n",
        "        return fused_features\n"
      ],
      "metadata": {
        "id": "G8XUGMCNK5yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Cross-Attention Mechanism: Cross-attention computes a similarity matrix between source and target features, using these similarities to fuse features.\n",
        "\n",
        "Residual Connection: Adds the original source features to the fused output, allowing the model to retain important source information.\n",
        "\n",
        "Feature Fusion Model: Combines features from both images in a structured way, providing more realistic motion transfer by aligning key details between source and target.\n",
        "\n",
        "Integration in Motion Transfer Model: Incorporate cross-attention into the motion transfer model’s intermediate layers, allowing more precise feature alignment.\n",
        "\n"
      ],
      "metadata": {
        "id": "t4PRprZZK93F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Temporal Consistency Layer\n",
        "A temporal consistency layer helps reduce jitter across consecutive frames by enforcing consistency in the feature space, ensuring smoother transitions.\n",
        "\n",
        "Implementation Code:"
      ],
      "metadata": {
        "id": "may3JVgVLFyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalConsistencyLayer(nn.Module):\n",
        "    def __init__(self, alpha=0.1):\n",
        "        super(TemporalConsistencyLayer, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, current_frame_features, previous_frame_features):\n",
        "        # Temporal consistency regularization\n",
        "        consistency_loss = nn.functional.mse_loss(current_frame_features, previous_frame_features)\n",
        "        return self.alpha * consistency_loss\n",
        "\n",
        "# Example usage in training loop\n",
        "temporal_consistency_layer = TemporalConsistencyLayer(alpha=0.1)\n",
        "\n",
        "# Assume frames is a list of feature maps from consecutive frames\n",
        "temporal_loss = 0.0\n",
        "for i in range(1, len(frames)):\n",
        "    temporal_loss += temporal_consistency_layer(frames[i], frames[i-1])\n",
        "\n",
        "total_loss = base_loss + temporal_loss\n"
      ],
      "metadata": {
        "id": "8Us36sfaLIok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "Temporal Consistency Regularization: Computes MSE loss between consecutive frame feature maps, enforcing stability and reducing flicker.\n",
        "\n",
        "Adjustable Weight (alpha): Controls the influence of temporal consistency in the total loss, allowing balance between smoothness and frame independence.\n",
        "\n",
        "Integration in Training Loop: Apply the temporal consistency layer on intermediate or final feature maps of consecutive frames to improve video smoothness."
      ],
      "metadata": {
        "id": "uteRr3p4LJPG"
      }
    }
  ]
}